{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required \n",
    "input_bq_goolge_project: str = None\n",
    "\n",
    "# Optional\n",
    "input_workflow_id: str = None\n",
    "input_submission_id: str = None\n",
    "input_workspace_namespace: str = None\n",
    "input_workspace_name: str = None\n",
    "input_task_name: list = None\n",
    "input_target_shard: int = None\n",
    "input_days_back_upper_bound: int = None  # The number of days back from today that the workflow started.\n",
    "input_days_back_lower_bound: int = None  # The number of days back from today that the workflow ended.\n",
    "output_bucket: str = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot Resource Monitoring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook visualizes data from a BQ dataset that holds workflow resource usage monitoring data. Please refer to the Readme for what data are collected and how they are collected. The notebook will produce three plots, one PDF and two interactive HTML reports.\n",
    "\n",
    "The following need to be provided to the notebook: \n",
    "- BQ Google Project: The google project that holds the BQ dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T22:15:28.346137Z",
     "start_time": "2024-02-06T22:15:25.762838Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade pip\n",
    "!pip uninstall -y cromwellMonitor\n",
    "!pip install --no-cache-dir git+https://github.com/broadinstitute/cromwell-task-monitor-bq-vis.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T22:15:29.474053Z",
     "start_time": "2024-02-06T22:15:28.346691Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 200) # so we can see all df rows\n",
    "\n",
    "from cromonitor.query.queryBQ import QueryBQToMonitor\n",
    "from cromonitor.plotting import plotting as plotUtils\n",
    "from cromonitor.table import utils as tableUtils\n",
    "from cromonitor.jupyter import utils as jupyterUtils\n",
    "from cromonitor.fiss import utils as fissUtils \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_namespace = input_workspace_namespace if input_workspace_namespace else os.environ[\"WORKSPACE_NAMESPACE\"]\n",
    "workspace_name = input_workspace_name if input_workspace_name else os.environ[\"WORKSPACE_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select Submission and Workflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if input_submission_id is None:\n",
    "    ###\n",
    "    # Get a list of submission for the workspace\n",
    "    ###\n",
    "    submissions_df_sorted = fissUtils.get_list_of_submissions(\n",
    "        workspace_namespace = workspace_namespace,\n",
    "            workspace_name= workspace_name,\n",
    "    )\n",
    "    display(submissions_df_sorted)\n",
    "    print(\"Select Submission ID Below\")\n",
    "    ###\n",
    "    # Select Submission Id\n",
    "    ###\n",
    "    sumission_ids = submissions_df_sorted['submissionId'].tolist()\n",
    "    selected_submission_id = jupyterUtils.create_submission_selector(\n",
    "    options = sumission_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "if input_workflow_id is None:\n",
    "    ####\n",
    "    # Get all the workflow ids assoccaited with the submission\n",
    "    ####\n",
    "    workflow_id_df_sorted = fissUtils.get_submission_workflow_ids(\n",
    "            workspace_namespace=workspace_namespace,\n",
    "            workspace_name=workspace_name,\n",
    "            submission_id=selected_submission_id.value,\n",
    "    )\n",
    "    display(workflow_id_df_sorted)\n",
    "    print(\"Select Workflow ID Below\")\n",
    "    ###\n",
    "    # Select Workflow Id \n",
    "    ###\n",
    "    workflow_ids = workflow_id_df_sorted['workflowId'].tolist()\n",
    "    selected_workflow_id = jupyterUtils.create_workflow_selector(\n",
    "    options = workflow_ids\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####\n",
    "# Create a workflow object to retrieve and store the selected workflow information\n",
    "####\n",
    "selected_workflow_info = fissUtils.Workflow(\n",
    "            workspace_namespace=workspace_namespace,\n",
    "            workspace_name=workspace_name,\n",
    "            submission_id= input_submission_id if input_submission_id else selected_submission_id.value,\n",
    "            parent_workflow_id=input_workflow_id if input_workflow_id else selected_workflow_id.value,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query BQ database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workflow_ids =  selected_workflow_info.subworkflow_ids + [selected_workflow_info.parent_workflow_id] \n",
    "PARENT_WORKFLOW_ID = selected_workflow_info.parent_workflow_id\n",
    "days_back_upper_bound = input_days_back_upper_bound if input_days_back_upper_bound else selected_workflow_info.days_from_workflow_start \n",
    "days_back_lower_bound = input_days_back_lower_bound if input_days_back_lower_bound else selected_workflow_info.days_from_workflow_end\n",
    "bq_goolge_project = input_bq_goolge_project  \n",
    "\n",
    "\n",
    "df_monitoring = QueryBQToMonitor(workflow_ids=workflow_ids, days_back_upper_bound=days_back_upper_bound, days_back_lower_bound=days_back_lower_bound, bq_goolge_project=bq_goolge_project)#, debug=True)\n",
    "metrics_filename = PARENT_WORKFLOW_ID + '_metrics_resource_monitoring.pkl'\n",
    "metadata_filename = PARENT_WORKFLOW_ID + '_metadata_runtime_resource_monitoring.pkl'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have two scenarios\n",
    "\n",
    "Scenario 1 : Perform a fresh query and save query results locally to be used in another session.\n",
    "Uses the QueryBQToMonitor class to query the BQ database using the variables that were provided for workflow_id and dates. \n",
    "After querying the BQ database, the data is saved locally to avoid the cost of querying the BQ again in the future. The next cell will save the pandas dataframe into a pickle file. (If data is saved locally you may skip this cell.)\n",
    "\n",
    "Scenario 2 : Import local query results that was saved from a earlier session. \n",
    "If resource data is saved locally from a previous run of this job then you will want to import them instead of rerunning the BQ query above. Run the next cell to import the pickle files saved from a previous session. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T22:15:41.607437Z",
     "start_time": "2024-02-06T22:15:29.925929Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists(metrics_filename) and os.path.exists(metadata_filename):\n",
    "    print(\"Loading data from local files\")\n",
    "    df_monitoring.metrics = tableUtils.load_dataframe(metrics_filename)\n",
    "    df_monitoring.metadata_runtime = tableUtils.load_dataframe(metadata_filename)\n",
    "else:\n",
    "    print(\"Loading data from querying BQ database\")\n",
    "    df_monitoring.query()\n",
    "    \n",
    "    ## Saves dataframe locally in pickle format\n",
    "    if not df_monitoring.metrics.empty and not df_monitoring.metrics.empty:\n",
    "        df_monitoring.metrics.to_pickle(metrics_filename)\n",
    "        df_monitoring.metadata_runtime.to_pickle(metadata_filename)\n",
    "    else:\n",
    "        print(\"Empty Database: No Files Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the tables obtained from scenario 1 or 2 the next cell will create an addtional monitoring dataframe table that will be used later during ploting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T22:15:41.930121Z",
     "start_time": "2024-02-06T22:15:41.608463Z"
    }
   },
   "outputs": [],
   "source": [
    "#Create metrics_runtime table\n",
    "df_monitoring.metrics_runtime = tableUtils.create_metrics_runtime_table(metrics=df_monitoring.metrics, metadata_runtime=df_monitoring.metadata_runtime)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workflow duration Summary\n",
    "The next few cells will obtain and plot the workflow duration summary. Consisting of a table and plot of the workflow duration per task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T22:15:47.842503Z",
     "start_time": "2024-02-06T22:15:41.932643Z"
    }
   },
   "outputs": [],
   "source": [
    "plotUtils.generate_workflow_summary(\n",
    "    parent_workflow_id=PARENT_WORKFLOW_ID, \n",
    "    df_monitoring=df_monitoring,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task sharded summary and task detailed summary\n",
    "The next cells will create either a sharded summary or a task detailed summary\n",
    "- Task shard summary: This is used for any scattered tasks, it provides a high level view of resource usage for each shard of a task. For example: all the average cpu usage for each task will be ploted togther in a bar plot. If there is interest in looking at the resourse usage for a particular shard, use the \"target_shard\" parameter in \"plot_resource_usage\" function to plot resourses used over time. \n",
    "- Task detailed summary: When a task isn't scattered plots the resource usage for task over time will be displayed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T22:15:47.842885Z",
     "start_time": "2024-02-06T22:15:47.837102Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "####\n",
    "# Select task to plot\n",
    "####\n",
    "#Get an array of task names in workflow\n",
    "AllTaskNames = df_monitoring.metrics_runtime.runtime_task_call_name.unique()\n",
    "# Create the SelectMultiple widget\n",
    "task_selector = jupyterUtils.create_task_selector(AllTaskNames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T22:16:00.850435Z",
     "start_time": "2024-02-06T22:15:53.809179Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# After selecting the tasks, pass the selected tasks to the function\n",
    "selected_task = input_task_name if input_task_name else task_selector.value\n",
    "resource_plot = plotUtils.plot_resource_usage(\n",
    "    df_monitoring=df_monitoring, \n",
    "    parent_workflow_id=PARENT_WORKFLOW_ID, \n",
    "    task_names=selected_task,\n",
    "    plt_height=4000, \n",
    "    plt_width=1000,\n",
    "    target_shard=input_target_shard\n",
    ")\n",
    "resource_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the returned plot as a PDF\n",
    "task_header = \"_\".join(selected_task)\n",
    "\n",
    "plot_file_name = f\"{PARENT_WORKFLOW_ID}_{selected_workflow_info.workflow_name}_{task_header}_resource_monitoring.pdf\"\n",
    "plotUtils.save_plot_as_pdf(plot=resource_plot, filename=plot_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save files to a Google Bucket\n",
    "Here we will be saving the file produced by the notebook into an a google bucket. You'll need to set the google bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-06T22:15:47.843424Z",
     "start_time": "2024-02-06T22:15:47.837415Z"
    }
   },
   "outputs": [],
   "source": [
    "#Requires that user (or Terra user proxy) has edit access to destination bucket\n",
    "OUTPUT_BUCKET = output_bucket if output_bucket else os.environ[\"WORKSPACE_BUCKET\"]+\"/\"+\"workflow_monitoring\"\n",
    "\n",
    "!gsutil cp ./{plot_file_name} {OUTPUT_BUCKET}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
